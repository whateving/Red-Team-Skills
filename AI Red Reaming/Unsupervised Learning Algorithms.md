# Unsupervised Learning Algorithms (The "Explorer")

**Definition:** Unsupervised learning algorithms explore **unlabeled data**. Their goal is not to predict a specific output (like "yes/no") but to discover hidden structures, patterns, and relationships within the data.

> ** The Core Difference:**
> * **Supervised Learning:** Learns from "correct answers" (labeled data).
> * **Unsupervised Learning:** Operates without an answer key (unlabeled data).

---

## 1. The Mental Model: "The New City"
Think of unsupervised learning like **exploring a new city without a map or GPS.**

* **Observation:** You look around and notice landmarks.
* **Pattern Recognition:** You realize all the restaurants are in one area, and the factories are in another.
* **Connection:** You figure out how streets connect different neighborhoods.
* **Result:** You build a mental map of the city's structure without anyone telling you the names of the streets.

---

## 2. The Three Main Problem Types
Unsupervised learning generally solves one of three types of problems:

### A. Clustering (The Organizer)
Grouping similar data points together based on their characteristics.
* *Real-world example:* Grouping a library of books by genre, or segmenting customers based on purchasing habits.
* *Goal:* Maximize similarity within a group, maximize difference between groups.

### B. Dimensionality Reduction (The Summarizer)
Reducing the number of variables (features) while keeping the important information.
* *Real-world example:* Summarizing a 100-page report into a 1-page abstract, or compressing a high-resolution image.
* *Goal:* Simplify the data to make it easier to process without losing the "story."

### C. Anomaly Detection (The Watchdog)
Identifying data points that are weird or deviate from the norm.
* *Real-world example:* Spotting a fake bill in a stack of real money, or flagging a credit card purchase made in a country you have never visited.
* *Goal:* Catch errors, fraud, or rare events.

---

## 3. How It Works: Core Mechanisms

Since the algorithm has no labels, it relies on **Mathematics of Similarity**. It calculates how "close" or "far" data points are from each other.

### Measuring Similarity (Distance Metrics)
The choice of "ruler" changes the results:
* **Euclidean Distance:** The straight-line distance between two points (as the crow flies).
* **Manhattan Distance:** The distance if you had to walk along a grid of city blocks (no diagonals).
* **Cosine Similarity:** Measures the angle between two points. (Are they pointing in the same direction? Useful for text analysis).

### Evaluating Clusters (Is the grouping good?)
Since we don't have the "right" answer, we check the geometry of the clusters:
1.  **Cohesion:** Are the points in the cluster tight and close together? (Good)
2.  **Separation:** Is there a nice big gap between this cluster and the next one? (Good)

---

## 4. Crucial Concepts & Vocabulary

### The Data
* **Unlabeled Data:** Data that has input features (like height, weight, color) but no target label (like "Dog" or "Cat"). The algorithm looks at the features to find the patterns.
* **Dimensionality:** The number of features (columns) in your dataset. Too many features lead to the **"Curse of Dimensionality,"** where data becomes sparse and distance calculations stop making sense.
* **Intrinsic Dimensionality:** The "true" number of variables needed to describe the data (often lower than the actual number of columns).

### The "Weird" Ones
* **Anomaly:** A point that deviates significantly from the norm (often implies a problem or event, like fraud).
* **Outlier:** A broader term for points far from the majority. These can be errors, anomalies, or just interesting edge cases.

### Data Prep (Mandatory)
* **Feature Scaling:** Because these algorithms use "distance" to measure similarity, you **must** scale your data.
    * *Why?* If one variable is "Salary" (0 to 1,000,000) and another is "Age" (0 to 100), the algorithm will think Salary is much more important just because the numbers are bigger.
    * *Techniques:* **Min-Max Scaling** (squish everything between 0 and 1) or **Standardization/Z-Score** (center around 0).


# K-Means Clustering (The "Grouper")

**Definition:** K-Means is a popular **unsupervised** algorithm that partitions a dataset into **$K$ distinct, non-overlapping clusters**.

**The Goal:**
* **Minimize Variance:** Make sure data points within a cluster are as similar (close) to each other as possible.
* **Maximize Separation:** Make sure different clusters are as distinct (far) from each other as possible.

> **Analogy:** Imagine a mixed bag of Skittles. You want to separate them into piles by color. You start by picking a few random spots on the table (centroids), then you drag every Skittle to the nearest pile. You keep adjusting the center of the piles until everything is perfectly sorted.

<img width="587" height="420" alt="image" src="https://github.com/user-attachments/assets/9ff6a027-f695-432f-ad88-88a86818d48d" />

---

## 1. How It Works: The Iterative Loop

K-Means is an **iterative** algorithm (it repeats a cycle until it gets the answer right).

### Step 1: Initialization
Randomly select **$K$** data points to act as the initial "Centroids" (the center point of a cluster).

### Step 2: Assignment
Look at every data point in the dataset. Assign it to the **nearest** Centroid based on distance (usually Euclidean).

### Step 3: Update
Calculate the **mean (average)** of all the points currently assigned to a cluster. Move the Centroid to this new average position.

### Step 4: Iteration
Repeat Steps 2 and 3 until the Centroids stop moving (convergence) or you reach a maximum number of tries. Once the centroids stabilize, re-assigning points yields the same result, so the algorithm has found a solution

---

## 2. The Math: Measuring "Closeness"

How does the algorithm know which cluster is "nearest"? It uses a distance metric. The most common is **Euclidean Distance**.

**The Formula:**
It calculates the straight-line distance between two points ($x$ and $y$) in multi-dimensional space.

$$d(x, y) = \sqrt{\sum (x_i - y_i)^2}$$

* $x_i$: Value of feature $i$ for point $x$
* $y_i$: Value of feature $i$ for point $y$



---

## 3. Choosing the "Optimal K"

How do you know how many clusters ($K$) you should have? (e.g., Should we group customers into 3 segments or 5?)

### Method A: The Elbow Method (Visual)
1.  Run K-Means for a range of $K$ (e.g., 1 to 10).
2.  Calculate the **WCSS** (Within-Cluster Sum of Squares) for each value of K. The WCSS measures the total variance within each cluster. Lower WCSS values indicate that the data points within clusters are more similar.
3.  **Plot WCSS vs. K.**
4.  **Look for the "Elbow":** The point where the WCSS starts dropping slowly. This is the sweet spot between "too simple" and "overfitting."

The elbow indicates the point of diminishing returns, where adding more clusters ($K$) no longer yields a significant drop in variance (WCSS).

<img width="562" height="455" alt="image" src="https://github.com/user-attachments/assets/75297ed8-cf32-4df1-bebf-472ac055476a" />


### Method B: Silhouette Analysis (Quantitative)
Measures how similar a point is to its *own* cluster compared to *other* clusters.
* **Range:** -1 to +1
* **Close to +1:** Perfect match. Well separated from neighbors.
* **Close to 0:** On the border/decision boundary.
* **Close to -1:** Wrong cluster assignment.
* **Strategy:** Choose the $K$ with the **highest average Silhouette Score**.

<img width="360" height="256" alt="image" src="https://github.com/user-attachments/assets/b2d67e0e-61cd-4da9-a00e-b96adccc7317" />


### Method C: Domain Expertise
Sometimes math isn't enough. If your marketing team can only manage 3 distinct campaigns, then $K=3$ is the practical choice, regardless of what the Elbow plot says.

Other factors to consider include:

* **Computational Cost:** Higher values of K generally require more computational resources.
* **Interpretability:** The resulting clusters should be meaningful and interpretable in the context of the problem.

---

## 4. Critical Data Assumptions

K-Means is powerful, but it has strict requirements to work correctly.

1.  **Spherical Clusters:** K-Means assumes clusters are round blobs. It struggles with complex shapes (like crescents or rings).
2.  **Similar Sizes:** It assumes clusters are roughly the same size/density.
3.  **Feature Scaling (Crucial):** Because it uses distance (Euclidean), you **must** normalize or standardize your data. If one feature is measured in millions and another in decimals, the millions will dominate the distance calculation.
4.  **Sensitivity to Outliers:** One bad outlier can pull the mean (Centroid) way off, ruining the cluster.


# Principal Component Analysis (PCA) (The "Simplifier")

**Definition:** PCA is a **dimensionality reduction** technique. It takes a dataset with many features (high dimensions) and flattens it into fewer features (lower dimensions) while keeping the most important information.

**The Goal:** Simplify the data without losing the "story."

> **Analogy: The Photographer**
> Imagine you are taking a picture of a teapot (a 3D object) to put in a 2D catalog.
> * If you take the photo from the top, it just looks like a circle (bad angle, lost information).
> * If you take it from the side, you see the handle, the spout, and the body (good angle, high information).
> * **PCA is the camera operator** finding the absolute best angle to take that 2D photo so you capture the maximum detail (variance) of the object.

<img width="2000" height="1400" alt="image" src="https://github.com/user-attachments/assets/56e81a67-f97a-4557-9bde-1ff2fdb79fcd" />

---

## 1. Core Concepts: The Vocabulary

To understand PCA, you need to grasp three specific terms:

### A. Variance (Information)
* **Definition:** How spread out the data is.
* **In PCA Context:** **Variance = Information.** If a feature has zero variance (everyone has the exact same value), it tells us nothing useful. PCA looks for the direction where the data is *most* spread out.

### B. Covariance (Relationships)
* **Definition:** How two variables change together.
* **In PCA Context:** PCA looks at the covariance matrix to see which features are correlated (redundant). If "Height" and "Leg Length" are highly correlated, PCA merges them into a single concept so you don't count the same information twice.

### C. Eigenvectors & Eigenvalues (The Direction & The Magnitude)
These are the mathematical engines of PCA.
* **Eigenvectors:** The **Direction**. These are the new axes (lines) PCA draws through the data. They point in the direction of the most spread.
* **Eigenvalues:** The **Value**. This number tells you *how much* variance (information) that specific Eigenvector holds.
    * *High Eigenvalue* = Very important direction (Keep it).
    * *Low Eigenvalue* = Noise/Unimportant (Drop it).

<img width="800" height="450" alt="image" src="https://github.com/user-attachments/assets/2ea9ddcc-2c99-4ced-8815-1578c5d66371" />

---

## 2. How PCA Works: The 6-Step Algorithm

### Step 1: Standardize the Data (Crucial!)
Scale everything so the mean is 0 and variance is 1.
* *Why?* If one variable is "Salary" (huge numbers like 100,000) and another is "Age" (small numbers like 30), the huge numbers will dominate the variance calculation incorrectly.

### Step 2: Calculate Covariance Matrix
Create a square matrix that summarizes how every variable relates to every other variable.

### Step 3: Compute Eigenvectors & Eigenvalues
Solve the equation $C \times v = \lambda \times v$ (see below) to find the new "directions" (vectors) and their "strength" (values).

### Step 4: Sort by Importance
Rank the Eigenvectors from highest Eigenvalue to lowest. The top one is "Principal Component 1" (PC1)—it holds the most information.

### Step 5: Select Top $k$ Components
Decide how many dimensions you want.
* *Example:* "I want to go from 100 features down to 3." -> Keep the top 3 Eigenvectors.

### Step 6: Transform (Project)
Re-orient the data onto these new axes.
* *Formula:* $Y = X \times V$ (Original Data $\times$ Selected Eigenvectors).

---

## 3. The Math: The Eigenvalue Equation

You solve this equation to find the components:

$$C \cdot v = \lambda \cdot v$$

* **$C$:** The Covariance Matrix (The data relationships).
* **$v$:** The Eigenvector (The new direction).
* **$\lambda$ (Lambda):** The Eigenvalue (The scaling factor/importance).

**Visual Example:**
Imagine a rubber band on a graph.
1.  **Vector $v$**: The rubber band lying on the x-axis $[1, 0]$.
2.  **Matrix $C$**: A transformation that stretches things by 2x.
3.  **Result**: The band is now $[2, 0]$.
4.  It points in the *same direction*, it just got longer. That makes it an **Eigenvector**.

<img width="287" height="201" alt="image" src="https://github.com/user-attachments/assets/89c6c45e-2965-4c74-be8a-866acd94ec6e" />

---

## 4. Choosing the Number of Components ($k$)

How do you know when to stop dropping dimensions?

**The "Explained Variance" Plot (Scree Plot):**
1.  Calculate the percentage of variance each PC explains.
2.  Sum them up (Cumulative Explained Variance).
3.  **The Rule of Thumb:** Pick $k$ such that you retain **95% (or 90%)** of the total variance.



---

## 5. Summary Table: PCA vs. Original Data

| Feature | Original Data | PCA Transformed Data |
| :--- | :--- | :--- |
| **Readability** | Easy to read (e.g., "Age", "Height") | Hard to interpret (e.g., "PC1", "PC2") |
| **Correlation** | Features might be highly correlated | Features are **uncorrelated** (orthogonal) |
| **Dimensionality** | High (potentially noisy) | Low (compact & efficient) |
| **Information** | 100% | Slightly less than 100% (lossy compression) |

---

## 6. Assumptions & Limitations

1.  **Linearity:** PCA assumes the data structure is linear. It draws straight lines. If your data looks like a spiral (Swiss Roll), PCA will fail (use t-SNE or UMAP instead).
2.  **Scale Sensitivity:** You **must** standardize data first.
3.  **Outliers:** PCA tries to capture *variance*. Extreme outliers have huge variance, so PCA will obsess over them, potentially skewing the results.


# Anomaly Detection (The "Watchdog")

**Definition:** Anomaly detection (or Outlier Detection) is an **unsupervised** technique used to identify data points that behave very differently from the "normal" crowd.

**The Goal:** Find the "odd ones out." These often represent critical events like fraud, system errors, or medical issues.

<img width="933" height="516" alt="image" src="https://github.com/user-attachments/assets/e4025225-74d2-4deb-97d6-6237c215d5b2" />

> **Analogy: The Security Guard**
> Imagine a security guard watching a building lobby.
> * **Normal:** People walk in at 9 AM and leave at 5 PM.
> * **Anomaly:** Someone climbs through a window at 3 AM.
> The guard doesn't need to know *who* the intruder is, just that their behavior doesn't fit the normal pattern.

---

## 1. The Three Types of Anomalies

Not all weird data is weird in the same way.

### A. Point Anomalies (Global Outliers)
A single data point that is just way off the charts.
* *Example:* A credit card transaction for $50,000 when the user usually spends $50.
* *Visual:* A red dot floating far away from the main blue cluster.

### B. Contextual Anomalies (Conditional)
Data that is normal in one situation but weird in another.
* *Example:* Wearing a swimsuit is normal at the beach (Context A), but anomalous at a business meeting (Context B).
* *Example:* 30°C temperature is normal in July, but an anomaly in December.

### C. Collective Anomalies (Group)
Individual points might look normal, but *together* they form a strange pattern.
* *Example:* Trying to log into a bank account is normal. Trying 500 times in one second is an anomaly (Cyberattack).

---

## 2. Common Techniques

### Statistical Methods
Assume data follows a shape (like a Bell Curve). Anything too far from the average (e.g., outside the 3rd standard deviation) is an outlier.
* *Tools:* Z-Score, Boxplots.

### Clustering Methods
Group the data. Any point that refuses to join a group, or forms a tiny, lonely group, is an outlier.
* *Tools:* K-Means (looking for small clusters), DBSCAN.

### Machine Learning Methods
Algorithms specifically built to hunt for differences.
* *Tools:* One-Class SVM, Isolation Forest, Local Outlier Factor (LOF).

---

## 3. Deep Dive: The Algorithms

### A. One-Class SVM (The "Fence Builder")
This algorithm draws a tight boundary (fence) around the "normal" data.
* **Mechanism:** It treats the origin (0,0) as the only "negative" data point and tries to separate all your data from it.
* **Result:** Anything inside the fence is Safe (Normal). Anything outside is an Intruder (Anomaly).
* **Best for:** Complex, non-linear data boundaries.

<img width="1400" height="449" alt="image" src="https://github.com/user-attachments/assets/d66f8cd5-e70b-4ae4-a232-6bc374b692b9" />


### B. Isolation Forest (The "20 Questions" Game)
This algorithm tries to isolate every single point by randomly slicing the data.
* **The Logic:** Anomalies are "few and different." Because they are different, it is **easier** to separate them from the crowd.
* **The Process:**
    1. Randomly pick a feature and a split value.
    2. Repeat until the point is alone.
    3. Count how many cuts it took.
* **The Score:**
    * **Short Path (Few cuts):** Easy to isolate = **Anomaly**.
    * **Long Path (Many cuts):** Hard to isolate (buried in the crowd) = **Normal**.

<img width="1318" height="792" alt="image" src="https://github.com/user-attachments/assets/35c73284-5b3c-45ba-934a-67969d07e28d" />


### C. Local Outlier Factor - LOF (The "Density Checker")
This compares the density of a point to the density of its neighbors.
* **The Logic:** If I am standing in a crowd, my "density" is high. If I am standing alone in a field, my "density" is low.
* **Mechanism:** It looks at the $k$-nearest neighbors.
    * If a point has a much **lower density** than its neighbors, it is an outlier.
* **Formula Insight:**
    $$LOF(p) \approx \frac{\text{Density of Neighbors}}{\text{Density of Point } p}$$
    * **LOF > 1:** Anomaly (I am less dense than my neighbors).
    * **LOF ≈ 1:** Normal (I am similar to my neighbors).

<img width="640" height="480" alt="image" src="https://github.com/user-attachments/assets/a14b6de5-bb36-4324-bf1d-18d715ecf218" />

---

## 4. Summary of Scores

| Algorithm | Metric | Interpretation |
| :--- | :--- | :--- |
| **Isolation Forest** | Anomaly Score (0 to 1) | **Close to 1:** Anomaly<br>**Close to 0.5:** Normal |
| **LOF** | LOF Score | **Significantly > 1:** Anomaly<br>**~ 1:** Normal |
| **Z-Score** (Stat) | Standard Deviations | **> 3 or < -3:** Typically an Anomaly |

---

## 5. Critical Data Assumptions

1.  **Imbalance:** Anomaly detection assumes anomalies are rare (e.g., 1% of the data). If 50% of your data is "anomalous," the algorithms will fail to find a "normal" baseline.
2.  **Feature Relevance:** If you don't include the right features (e.g., you check "time of day" but not "transaction amount" for fraud), you won't find the anomaly.
3.  **Distribution:** Statistical methods assume a Normal (Gaussian) distribution. If your data is skewed, Z-scores might mislead you.
