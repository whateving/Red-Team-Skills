# üêç Python Libraries for AI (The Toolbox)

Python is the king of AI because of its libraries. Instead of writing complex math from scratch, we use pre-built tools.

---

## 1. Scikit-learn (The Classic Machine Learning Tool) üõ†Ô∏è

**Definition:** Scikit-learn is the standard library for **traditional Machine Learning** (everything *except* deep neural networks). It is built on top of NumPy and Matplotlib.

### A. What can it do?
1.  **Supervised Learning:** Algorithms that learn from labeled data.
    * *Examples:* Linear Regression, Support Vector Machines (SVM), Random Forests.
2.  **Unsupervised Learning:** Algorithms that find patterns in unlabeled data.
    * *Examples:* K-Means Clustering, PCA (Dimensionality Reduction).
3.  **Data Preprocessing:** Cleaning and preparing raw data (Crucial Step!).

### B. Key Preprocessing Tools
Raw data is messy. Scikit-learn fixes it.

| Tool | Purpose | Example |
| :--- | :--- | :--- |
| **StandardScaler** | Scales numbers so they have a mean of 0. | Turning variable heights (cm vs ft) into a standard scale. |
| **MinMaxScaler** | Squashes numbers between 0 and 1. | Good for image pixel intensity. |
| **OneHotEncoder** | Converts text categories into binary math. | "Red, Blue" $\rightarrow$ `[1, 0]`, `[0, 1]` |
| **SimpleImputer** | Fills in missing data. | Replaces "NaN" with the average value. |

### C. The "Fit-Predict" Workflow üîÑ
Scikit-learn uses a beautiful, consistent 3-step pattern for almost every algorithm:

1.  **Instantiate:** Pick your model.
    ```python
    model = LogisticRegression()
    ```
2.  **Fit (Train):** Teach the model using training data.
    ```python
    model.fit(X_train, y_train)
    ```
3.  **Predict:** Ask the model to guess on new data.
    ```python
    prediction = model.predict(X_test)
    ```



---

## 2. PyTorch (The Deep Learning Powerhouse) üî•

**Definition:** PyTorch (by Meta AI) is a library designed for **Deep Learning** and **Neural Networks**.

### A. Key Features
1.  **Tensors:** The building block of PyTorch. They are like NumPy arrays (tables of numbers) but **run on GPUs** for massive speed acceleration.
2.  **Dynamic Graphs:** You can change the network structure *while* it runs. This makes debugging much easier than competitors like TensorFlow.
3.  **Autograd:** It automatically does the calculus (gradients) for you. You define the "Forward" pass, and it figures out the "Backward" pass automatically.

### B. Building a Model (The Lego Block Approach)
You build neural networks by stacking layers.

* **Sequential:** Stack layers in a straight line.
    ```python
    model = nn.Sequential(
        nn.Linear(784, 128),  # Input layer
        nn.ReLU(),            # Activation
        nn.Linear(128, 10)    # Output layer
    )
    ```
* **Custom Module:** Create a class for complex networks (most common in pro work).

### C. The Training Loop üèãÔ∏è
Unlike Scikit-learn's one-line `.fit()`, PyTorch requires you to write the training loop explicitly.

**The 5 Steps of a PyTorch Loop:**
1.  **Forward Pass:** Pass data through the model.
2.  **Calculate Loss:** Check how wrong the model was (e.g., `CrossEntropyLoss`).
3.  **Zero Gradients:** Clear old calculations (`optimizer.zero_grad()`).
4.  **Backward Pass:** Calculate the updates needed (`loss.backward()`).
5.  **Step:** Update the weights (`optimizer.step()`).

### D. Data Loading
PyTorch uses two specific tools to handle massive datasets:
* **Dataset:** Stores your data samples.
* **DataLoader:** Shuffles the data and feeds it to the model in small batches (to prevent crashing memory).

---

## Summary: When to use which? ü§î

| Library | Best For... |
| :--- | :--- |
| **Scikit-learn** | **Tabular Data** (Excel sheets), simple classification, regression, and data cleaning. |
| **PyTorch** | **Complex Data** (Images, Audio, Text), Deep Learning, and custom Neural Networks. |
